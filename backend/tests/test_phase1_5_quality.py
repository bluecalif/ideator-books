"""Phase 1.5 í’ˆì§ˆ ê°œì„  í†µí•© í…ŒìŠ¤íŠ¸ - ì‹¤ì œ ë°ì´í„°"""
import sys
import io
from pathlib import Path
import logging
from datetime import datetime
from dotenv import load_dotenv

# UTF-8 ì¶œë ¥ ê°•ì œ (Windows PowerShell ì¸ì½”ë”© ë¬¸ì œ í•´ê²°)
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# .env íŒŒì¼ ë¡œë“œ
env_path = project_root / ".env"
load_dotenv(dotenv_path=env_path)

from backend.services.kb_service import kb_service
from backend.services.book_service import book_service
from backend.langgraph_pipeline.state import create_initial_state
from backend.langgraph_pipeline.graph import graph
from backend.core.models_config import models_config

# ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸° (íŒŒì¼ëª…ì— ì‚¬ìš©)
model_name = models_config.PRODUCER_MODEL.replace('/', '_').replace('.', '_')

# ë¡œê¹… ì„¤ì •
log_file = Path(__file__).parent / "output" / f"test_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
log_file.parent.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)


def test_phase1_5_quality():
    """Phase 1.5 í’ˆì§ˆ ê°œì„  í†µí•© í…ŒìŠ¤íŠ¸"""
    
    print("\n" + "="*80)
    print(f"Phase 1.5 í’ˆì§ˆ ê°œì„  í†µí•© í…ŒìŠ¤íŠ¸ - {models_config.PRODUCER_MODEL}")
    print("="*80)
    print(f"\n[MODELS]")
    print(f"   - AnchorMapper: {models_config.ANCHOR_MAPPER_MODEL}")
    print(f"   - Reviewer: {models_config.REVIEWER_MODEL}")
    print(f"   - Integrator: {models_config.INTEGRATOR_MODEL}")
    print(f"   - Producer: {models_config.PRODUCER_MODEL}")
    
    # 1. KB ë¡œë“œ
    print("\n[1/5] KB ë¡œë“œ ì¤‘...")
    kb_result = kb_service.load_all_domains()
    print(f"[OK] KB ë¡œë“œ ì™„ë£Œ: {sum(kb_result.values())}ê°œ ì•„ì´í…œ")
    
    # í†µí•©ì§€ì‹ í™•ì¸
    integrated_items = [item for item in kb_service.all_items if item.is_integrated_knowledge]
    print(f"   - í†µí•©ì§€ì‹: {len(integrated_items)}ê°œ")
    for domain in kb_service.DOMAINS:
        domain_integrated = [i for i in integrated_items if i.domain == domain]
        print(f"     â€¢ {domain}: {len(domain_integrated)}ê°œ")
    
    # 2. CSV ë¡œë“œ
    print("\n[2/5] CSV ë¡œë“œ ì¤‘...")
    csv_path = project_root / "docs" / "100ê¶Œ ë…¸ì…˜ ì›ë³¸_ìˆ˜ì •.csv"
    
    if not csv_path.exists():
        print(f"[FAIL] CSV íŒŒì¼ ì—†ìŒ: {csv_path}")
        return False
    
    book_service.csv_path = csv_path
    book_count = book_service.load_books()
    
    if book_count == 0:
        print(f"[FAIL] CSV ë¡œë“œ ì‹¤íŒ¨")
        return False
    
    print(f"[OK] ë„ì„œ ë¡œë“œ ì™„ë£Œ: {book_count}ê¶Œ")
    
    # í†µê³„ ì¶œë ¥
    stats = book_service.get_stats()
    print(f"   - ë„ë©”ì¸ë³„: {stats['by_domain']}")
    
    # í…ŒìŠ¤íŠ¸ìš© ì±… ì„ íƒ (ì²« ë²ˆì§¸ ì±…)
    test_book_raw = book_service.books[0]
    
    # í•„ë“œ ë§¤í•‘ (CSV ì»¬ëŸ¼ëª… â†’ í…ŒìŠ¤íŠ¸ìš©)
    test_book = {
        'id': test_book_raw.get('ì¼ë ¨ë²ˆí˜¸', 1),
        'title': test_book_raw.get('Title', 'ì œëª© ì—†ìŒ'),
        'author': test_book_raw.get('ì €ì', 'ì €ì ë¯¸ìƒ'),
        'topic': test_book_raw.get('Topic', 'ì£¼ì œ ì—†ìŒ'),
        'summary': test_book_raw.get('ìš”ì•½', 'ìš”ì•½ ì—†ìŒ'),  # 'ìš”ì•½' ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        'domain': test_book_raw.get('êµ¬ë¶„', 'ë¯¸ë¶„ë¥˜')
    }
    
    print(f"\nğŸ“š í…ŒìŠ¤íŠ¸ ë„ì„œ:")
    print(f"   - ID: {test_book['id']}")
    print(f"   - ì œëª©: {test_book['title']}")
    print(f"   - ì €ì: {test_book['author']}")
    print(f"   - ë„ë©”ì¸: {test_book['domain']}")
    print(f"   - ì£¼ì œ: {test_book['topic']}")
    print(f"   - ìš”ì•½ ê¸¸ì´: {len(test_book['summary'])} chars")
    
    # 3. 1p ìƒì„± (LangGraph ì‹¤í–‰)
    print("\n[3/5] 1p ìƒì„± ì¤‘...")
    print("   (ì´ ê³¼ì •ì€ ì•½ 1-2ë¶„ ì†Œìš”ë©ë‹ˆë‹¤...)")
    
    # State ìƒì„±
    initial_state = create_initial_state(
        book_ids=[test_book['id']],
        mode="reduce",  # Reduce ëª¨ë“œ í…ŒìŠ¤íŠ¸
        format="content",  # ì½˜í…ì¸ í˜•
        remind_enabled=False,
        book_summary=test_book['summary'],
        book_title=test_book['title'],
        book_author=test_book['author'],
        book_topic=test_book['topic']
    )
    
    # Config
    config = {"configurable": {"thread_id": f"test_{datetime.now().strftime('%Y%m%d_%H%M%S')}"}}
    
    # ì‹¤í–‰
    node_count = 0
    final_state = None
    producer_output = None  # Producer ì¶œë ¥ ë³„ë„ ì €ì¥
    validator_output = None  # Validator ì¶œë ¥ ë³„ë„ ì €ì¥
    
    try:
        for event in graph.stream(initial_state, config):
            node_name = list(event.keys())[0]
            node_count += 1
            
            print(f"   âœ“ Node {node_count}: {node_name}")
            logger.info(f"Node completed: {node_name}")
            
            # ë…¸ë“œë³„ ì£¼ìš” ì •ë³´ ë¡œê¹…
            node_data = event[node_name]
            
            # Producerì™€ Validator ì¶œë ¥ ì €ì¥
            if node_name == "producer":
                producer_output = node_data
            elif node_name == "validator":
                validator_output = node_data
            
            if node_name == "anchor_mapper":
                anchors = node_data.get("anchors", {})
                available_count = len(node_data.get("available_anchors", []))
                logger.info(f"  Anchors: {anchors}")
                logger.info(f"  Available anchors: {available_count}ê°œ")
                print(f"      - ì•µì»¤ ë§¤í•‘ ì™„ë£Œ: {len(anchors)}ê°œ ë„ë©”ì¸")
                print(f"      - ì‚¬ìš© ê°€ëŠ¥ ì•µì»¤: {available_count}ê°œ")
            
            elif "reviewer" in node_name.lower():
                reviews = node_data.get("reviews", [])
                if reviews:
                    review = reviews[-1]  # ë§ˆì§€ë§‰ ì¶”ê°€ëœ ë¦¬ë·°
                    logger.info(f"  Review ({review['domain']}): {len(review.get('raw_content', ''))} chars")
                    print(f"      - {review['domain']} ë¦¬ë·° ì™„ë£Œ")
            
            elif node_name == "integrator":
                tension_axes = node_data.get("tension_axes", [])
                logger.info(f"  Tension axes: {len(tension_axes)}ê°œ")
                for i, axis in enumerate(tension_axes, 1):
                    logger.info(f"    {i}. {axis}")
                print(f"      - ê¸´ì¥ì¶•: {len(tension_axes)}ê°œ ì¶”ì¶œ")
            
            elif node_name == "producer":
                onepager_length = len(node_data.get("onepager_md", ""))
                unique_count = len(node_data.get("unique_sentences", []))
                logger.info(f"  1p length: {onepager_length} chars")
                logger.info(f"  Unique sentences: {unique_count}ê°œ")
                print(f"      - 1p ìƒì„± ì™„ë£Œ: {onepager_length} chars")
                print(f"      - ê³ ìœ ë¬¸ì¥: {unique_count}ê°œ")
            
            elif node_name == "validator":
                anchored = node_data.get("anchored_by_percent", 0)
                unique = node_data.get("unique_sentence_count", 0)
                external = node_data.get("external_frame_count", 0)
                fake = node_data.get("fake_anchor_count", 0) if "fake_anchor_count" in str(node_data) else "N/A"
                passed = node_data.get("validation_passed", False)
                
                logger.info(f"  Validation: anchored={anchored:.1%}, unique={unique}, external={external}, fake={fake}")
                print(f"      - anchored_by: {anchored:.1%}")
                print(f"      - ê³ ìœ ë¬¸ì¥: {unique}ê°œ")
                print(f"      - ì™¸ë¶€í”„ë ˆì„: {external}ê°œ")
                print(f"      - ê°€ì§œì•µì»¤: {fake}ê°œ" if fake != "N/A" else "")
                print(f"      - ê²€ì¦: {'âœ… PASS' if passed else 'âŒ FAIL'}")
            
            # ìµœì¢… state ì €ì¥
            final_state = node_data
        
        print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ: {node_count}ê°œ ë…¸ë“œ")
        
    except Exception as e:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        logger.error(f"Pipeline error: {e}", exc_info=True)
        return False
    
    # 4. ê²°ê³¼ ì €ì¥
    print("\n[4/5] ê²°ê³¼ ì €ì¥ ì¤‘...")
    
    if not producer_output or "onepager_md" not in producer_output:
        print("âŒ Producer ì¶œë ¥ì´ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    onepager_md = producer_output["onepager_md"]
    
    if not onepager_md:
        print("âŒ 1pê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤")
        return False
    output_file = Path(__file__).parent / "output" / f"1p_{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"# í’ˆì§ˆ ê°œì„  í…ŒìŠ¤íŠ¸ ê²°ê³¼\n\n")
        f.write(f"**í…ŒìŠ¤íŠ¸ ì‹œê°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**ì‚¬ìš© ëª¨ë¸**: {models_config.PRODUCER_MODEL}\n\n")
        f.write(f"**í…ŒìŠ¤íŠ¸ ë„ì„œ**: {test_book['title']} ({test_book['author']})\n\n")
        f.write(f"---\n\n")
        f.write(onepager_md)
    
    print(f"âœ… 1p ì €ì¥ ì™„ë£Œ: {output_file}")
    print(f"   í¬ê¸°: {len(onepager_md)} chars")
    
    # 5. í’ˆì§ˆ ì ê²€ ë¦¬í¬íŠ¸
    print("\n[5/5] í’ˆì§ˆ ì ê²€ ë¦¬í¬íŠ¸")
    print("="*80)
    
    # 5.1 í†µí•©ì§€ì‹ ì‚¬ìš© ì—¬ë¶€
    integrated_anchor_count = onepager_md.count("í†µí•©ì§€ì‹")
    print(f"\nâœ“ í†µí•©ì§€ì‹ ì•µì»¤ ì‚¬ìš©: {integrated_anchor_count}íšŒ")
    
    # 5.2 1p ì œì•ˆì„œ êµ¬ì¡° í™•ì¸
    required_sections = [
        "# í˜•ì‹ ë¶„ê¸°",
        "# ë„ë©”ì¸ ë¦¬ë·° ì¹´ë“œ",
        "# í†µí•© ê¸°ë¡",
        "# ìµœì¢… 1p ì œì•ˆì„œ",
        "## ì œëª©",
        "## ë¡œê·¸ë¼ì¸",
        "## ëŒ€ìƒ",
        "## í•µì‹¬ ì•½ì†",
        "## í¬ë§·",
        "## êµ¬ì„±",
        "## ê³ ìœ  ë¬¸ì¥",
        "## CTA"
    ]
    
    print(f"\nâœ“ 1p ì œì•ˆì„œ êµ¬ì¡° í™•ì¸:")
    missing_sections = []
    for section in required_sections:
        if section in onepager_md:
            print(f"   âœ… {section}")
        else:
            print(f"   âŒ {section} (ëˆ„ë½)")
            missing_sections.append(section)
    
    # 5.3 ì•µì»¤ ì‚¬ìš© í†µê³„
    import re
    anchors_used = re.findall(r'\[([^\]]+)\]', onepager_md)
    unique_anchors = set(anchors_used)
    
    print(f"\nâœ“ ì•µì»¤ ì‚¬ìš© í†µê³„:")
    print(f"   - ì´ ì‚¬ìš©: {len(anchors_used)}íšŒ")
    print(f"   - ê³ ìœ  ì•µì»¤: {len(unique_anchors)}ê°œ")
    
    # 5.4 ê²€ì¦ ê²°ê³¼
    print(f"\nâœ“ ê²€ì¦ ê²°ê³¼:")
    validation_passed = validator_output.get("validation_passed") if validator_output else False
    validation_errors = validator_output.get("validation_errors", []) if validator_output else []
    
    if validation_passed:
        print("   âœ… ëª¨ë“  ê²€ì¦ í†µê³¼!")
    else:
        print("   âŒ ê²€ì¦ ì‹¤íŒ¨:")
        for error in validation_errors:
            print(f"      - {error}")
    
    # ìµœì¢… ê²°ê³¼
    print("\n" + "="*80)
    print("ğŸ“Š ìµœì¢… í‰ê°€")
    print("="*80)
    
    # Validator ì¶œë ¥ì—ì„œ ê²€ì¦ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    anchored_by_percent = validator_output.get("anchored_by_percent", 0) if validator_output else 0
    unique_sentence_count = validator_output.get("unique_sentence_count", 0) if validator_output else 0
    external_frame_count = validator_output.get("external_frame_count", 0) if validator_output else 0
    
    success_criteria = [
        (integrated_anchor_count > 0, "í†µí•©ì§€ì‹ ì•µì»¤ ì‚¬ìš©"),
        (len(missing_sections) == 0, "1p ì œì•ˆì„œ êµ¬ì¡° ì™„ì„±"),
        (anchored_by_percent >= 0.9, "ì•µì»¤ ì»¤ë²„ë¦¬ì§€ 90% ì´ìƒ"),
        (unique_sentence_count >= 3, "ê³ ìœ ë¬¸ì¥ 3ê°œ ì´ìƒ"),
        (external_frame_count == 0, "ì™¸ë¶€ í”„ë ˆì„ì›Œí¬ 0ê°œ"),
        (validation_passed, "ìµœì¢… ê²€ì¦ í†µê³¼")
    ]
    
    passed_count = sum(1 for passed, _ in success_criteria if passed)
    total_count = len(success_criteria)
    
    for passed, criterion in success_criteria:
        status = "âœ…" if passed else "âŒ"
        print(f"{status} {criterion}")
    
    print(f"\nğŸ¯ ë‹¬ì„±ë¥ : {passed_count}/{total_count} ({passed_count/total_count*100:.0f}%)")
    
    print(f"\nğŸ“ ì¶œë ¥ íŒŒì¼:")
    print(f"   - 1p: {output_file}")
    print(f"   - ë¡œê·¸: {log_file}")
    
    print("\n" + "="*80)
    
    if passed_count == total_count:
        print("âœ… ëª¨ë“  í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í–ˆìŠµë‹ˆë‹¤!")
        return True
    else:
        print(f"âš ï¸  {total_count - passed_count}ê°œ ê¸°ì¤€ ë¯¸ë‹¬ì„±")
        return False


if __name__ == "__main__":
    print("\n[START] Phase 1.5 Quality Test\n")
    
    success = test_phase1_5_quality()
    
    if success:
        print("\n[SUCCESS] All quality criteria met!")
        print("\nNext steps:")
        print("1. Check .md file in output directory for 1p content")
        print("2. Check .log file for node details")
        print("3. Reply 'OK' if satisfied")
        print("4. Request improvements if needed")
    else:
        print("\n[WARN] Some quality criteria not met")
        print("Improvements needed.")
    
    sys.exit(0 if success else 1)

